<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xi Xiao</title>

    <!-- Last Updated -->
    <div style="color:#555; font-size:15px; margin-bottom:18px;">
      Last updated @ January 16, 2026
    </div>

    <meta name="author" content="Xi Xiao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <!-- ================= HEADER ================= -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <!-- text -->
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xi Xiao
                </p>
                <p>
                  Hiüëã I'm Xi Xiao, a third-year Ph.D. student in <a href="https://www.uab.edu/cas/computerscience/">Computer Science at the University of Alabama at Birmingham</a>, cd-advised by
                  <a href="https://wangt0716.github.io/index.html">Prof. Tianyang Wang</a> and <a href="https://xulabs.github.io/min-xu/">Prof. Min Xu</a> from <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. I'm also a student researcher at
                  <a href="https://www.ornl.gov">Oak Ridge National Laboratory</a> working with
                  <a href="https://www.ornl.gov/staff-profile/xiao-wang">Dr. Xiao Wang</a> since May 2025.
                </p>
                <p>
                  I am a core contributor to <b>ORBIT-2</b>, an exascale climate foundation model trained on the 
                  Frontier supercomputer. The work received the <b>Best Paper Award</b> at SC 2025 and is a
                  <b>ACM Gordon Bell Prize finalist</b>. ORBIT-2 has since been integrated into NVIDIA‚Äôs
                  enterprise climate AI stack, powering large-scale forecasting services that reach
                  millions of users worldwide.
                </p>


                

                
                <p style="text-align:center">
                  <a href="mailto:xxiao@uab.edu">Email</a> &nbsp;/&nbsp;
                  <a href="assets/Xi_Xiao_s_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Q4DnRVgAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/xixiaouab">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/xi-xiao-4800272a5">LinkedIn</a>
                </p>
              </td>

              <!-- photo -->
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/XiXiao.jpg">
                  <img style="width:100%;max-width:100%;object-fit: cover;border-radius: 50%;"
                       alt="profile photo" src="images/XiXiao.jpg" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </tbody></table>

          <!-- ================= NEWS (two-column, aligned) ================= -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">

                <h2>News</h2>

                <table style="width:100%;border-collapse:collapse;">
                  <tbody>




                    
                    <!-- 2026 -->

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>01/2026</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>Transactions on Machine Learning Research</b>!</td>
                    </tr>


                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>01/2026</b></td>
                      <td style="padding:2px 0;">Two papers accepted by <b>ICASSP 2026</b>!</td>
                    </tr>

                    
                    


                    
                    <!-- 2025 -->
                    <tr>
                      <td style="padding:2px 0; width:110px; white-space:nowrap;"><b>12/2025</b></td>
                      <td style="padding:2px 0;">üéâ Received a <b>Tinker Research Grant ($5,000)</b> from 
                        <a href="https://thinkingmachines.ai/">Thinking Machines Lab</a> to support my post-training related research!</td>
                    </tr>

                   
                    
                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>11/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <span style="color:#d00;"><b>SC 2025 ‚Äî Best Paper Award üèÜ</b></span> Deeply honored to be the only student author.</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>11/2025</b></td>
                      <td style="padding:2px 0;">üéâ <b>Big News: Our Work Was Featured by AMD!</b> <a href="https://www.amd.com/en/blogs/2025/earth-system-modeling-with-orbit-2.html">Read the story</a></td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>11/2025</b></td>
                      <td style="padding:2px 0;">Two papers accepted by <b>WACV 2026</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>11/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>AAAI 2026</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>10/2025</b></td>
                      <td style="padding:2px 0;">Our new survey <i>Prompt-based Adaptation in Large-scale Vision Models: A Survey üöÄ</i> is released on arXiv.</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>09/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>NeurIPS 2025</b>! See you in San Diego!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>09/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>WACV 2026</b>! <span style="color:#d00;"><b>Round 1 Acceptance (85/1329 ‚âà 6.4%)</b></span></td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>08/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>Findings of EMNLP 2025</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>07/2025</b></td>
                      <td style="padding:2px 0;">Our work selected as a finalist for the <span style="color:#d00;"><b>ACM Gordon Bell Prize üèÜ</b></span></td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>07/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>COLM 2025</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>07/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>ACM MM 2025</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>06/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>ICCV 2025</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>05/2025</b></td>
                      <td style="padding:2px 0;">One paper accepted by <b>ECML-PKDD 2025</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>05/2025</b></td>
                      <td style="padding:2px 0;">Joined the Computational Sciences and Engineering Division at <b>Oak Ridge National Laboratory</b> for a long-term research internship!</td>
                    </tr>

                    <!-- 2024 -->
                    <tr>
                      <td style="padding:6px 0 2px; white-space:nowrap;"><b>12/2024</b></td>
                      <td style="padding:6px 0 2px;">One paper accepted by <b>ICASSP 2025</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>03/2024</b></td>
                      <td style="padding:2px 0;">Serving as a Reviewer for <b>IEEE Transactions on Circuits and Systems for Video Technology</b>!</td>
                    </tr>

                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>01/2024</b></td>
                      <td style="padding:2px 0;">Started Ph.D. in Computer Science at the <b>University of Alabama at Birmingham</b>!</td>
                    </tr>

                  </tbody>
                </table>

              </td>
            </tr>
          </tbody></table>

          <!-- ================= RESEARCH ================= -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I have broad interests in computer vision, and language models.
                  My recent work focuses on the <b>post-training stage</b> of large-scale models (LLMs/MLLMs/LVMs), including 
                  <b>parameter-efficient fine-tuning (PEFT)</b>, <b>reinforcement learning‚Äìbased alignment</b>, and 
                  <b>model quantization</b>. I aim to build efficient and robust intelligent systems that can perform 
                  reliably in extreme scenarios with <i>limited data, limited compute, and limited storage resources</i>.
                  Some representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- ================= PAPERS ================= -->
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!-- ORBIT-2 -->
            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/orbit2.jpg"
                     alt="ORBIT-2"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</span><br>
                Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, <strong><u>Xi Xiao</u></strong>, 
                David Pugmire, Ming Fan, Nasik M. Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, 
                Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">SC</span>, 2025
                &nbsp; <font color="red"><strong>Best Paper Award, Gordon Bell Prize Finalist</strong></font><br>

                <a href="https://dl.acm.org/doi/10.1145/3712285.3771989">paper</a> /
                <a href="https://github.com/XiaoWang-Github">code</a> /
                <a href="https://www.amd.com/en/blogs/2025/earth-system-modeling-with-orbit-2.html">AMD story</a>
                <p></p>
                <p>
                  An exascale vision transformer for high-resolution climate downscaling on Frontier, enabling accurate and efficient prediction of regional weather and climate extremes.
                </p>
              </td>
            </tr>

            <!-- Prompt-based Adaptation Survey -->
            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/prompt_survey.jpg"
                     alt="Prompt-based Adaptation Survey"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">Prompt-based Adaptation in Large-scale Vision Models: A Survey</span><br>
                <strong><u>Xi Xiao</u></strong>, Yunbei Zhang, Lin Zhao, Yiyang Liu, et al.<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">TMLR</span>, 2026<br>
                <a href="https://arxiv.org/abs/2510.13219">paper</a> /
                <a href="https://github.com/yunbeizhang/Awesome-Visual-Prompt-Tuning">resources</a>
                <p></p>
                <p>
                  A comprehensive taxonomy and survey of visual prompt tuning and prompting for large vision models,
                  covering learnable, generative, and non-learnable prompts across diverse tasks.
                </p>
              </td>
            </tr>

            <!-- Visual Instance-aware Prompt Tuning -->
            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/vipt.jpg"
                     alt="Visual Instance-aware Prompt Tuning"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">Visual Instance-aware Prompt Tuning</span><br>
                <strong><u>Xi Xiao</u></strong>, Yunbei Zhang, Xingjian Li, Tianyang Wang, Xiao Wang, Yuxiang Wei, Jihun Hamm, Min Xu<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">ACM MM</span>, 2025<br>
                <a href="https://dl.acm.org/doi/10.1145/3746027.3754858">paper</a> /
                <a href="https://github.com/xixiaouab?tab=repositories">code</a>
                <p></p>
                <p>
                  Instance-aware visual prompts that adapt to each image, mitigating overfitting and improving transferability of ViT-based classifiers under distribution shift.
                </p>
              </td>
            </tr>

            <!-- MagicID (co-first author) -->
            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/magicid.jpg"
                     alt="MagicID"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization</span><br>
                Hengjia Li*, Lifan Jiang*, <strong><u>Xi Xiao*</u></strong>, Tianyang Wang, Hongwei Yi, Boxi Wu, Deng Cai<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">ICCV</span>, 2025
                &nbsp; <strong>(* equal contribution)</strong><br>
                <a href="https://echopluto.github.io/MagicID-project/">project page</a> /
                <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_MagicID_Hybrid_Preference_Optimization_for_ID-Consistent_and_Dynamic-Preserved_Video_Customization_ICCV_2025_paper.pdf">paper</a> /
                <a href="https://github.com/EchoPluto/MagicID">code</a>
                <p></p>
                <p>
                  A hybrid preference optimization framework that jointly preserves identity and motion dynamics for personalized text-to-video generation.
                </p>
              </td>
            </tr>

            <!-- CAD-VAE -->
            <tr>
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/cadvae.jpg"
                     alt="CAD-VAE"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">CAD-VAE: Leveraging Correlation-Aware Latents for Comprehensive Fair Disentanglement</span><br>
                Chenrui Ma, <strong><u>Xi Xiao</u></strong>, Tianyang Wang, Xiao Wang, Yanning Shen<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">AAAI</span>, 2026<br>
                <a href="https://arxiv.org/abs/2503.07938">paper</a> /
                <a href="https://github.com/merry7cherry/CAD-VAE">code</a>
                <p></p>
                <p>
                  A correlation-aware latent space that jointly improves disentanglement and fairness in generative models through causal regularization.
                </p>
              </td>
            </tr>

            <!-- MoRE-Brain -->
            <tr>
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/morebrain.jpg"
                     alt="MoRE-Brain"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding</span><br>
                Yuxiang Wei, Yanteng Zhang, <strong><u>Xi Xiao</u></strong>, Tianyang Wang, Xiao Wang, Vince D. Calhoun<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">NeurIPS</span>, 2025<br>
                <a href="https://openreview.net/forum?id=fYSPRGmS6l">paper</a> /
                <a href="https://github.com/yuxiangwei0808/MoRE-Brain">code</a>
                <p></p>
                <p>
                  A routed mixture-of-experts architecture for diffusion-based fMRI-to-image reconstruction, achieving strong cross-subject generalization and interpretable brain‚Äìmodel alignment.
                </p>
              </td>
            </tr>



            <!-- Sensitivity-lora -->
            <tr>
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/slora.jpg"
                     alt="Sensitivity-lora"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">Sensitivity-LoRA : Low-Load Sensitivity-Based Fine-Tuning for Large Language Models</span><br>
                Hao Zhang, Bo Huang, Zhenjia Li, <strong><u>Xi Xiao</u></strong>, Hui Yi Leong, Zumeng Zhang, Xinwei Long, Tianyang Wang, Hao Xu<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">Findings of EMNLP</span>, 2025<br>
                <a href="https://aclanthology.org/2025.findings-emnlp.709.pdf">paper</a> /
                <a href="#">code</a>
                <p></p>
                <p>
                  An efficient fine-tuning method that dynamically allocates ranks to weight matrices based on both their global and local sensitivities. It leverages the second-order derivatives (Hessian Matrix) of the loss function to effectively capture weight sensitivity, enabling optimal rank allocation with minimal computational overhead.
                </p>
              </td>
            </tr>



            <!-- MI2V -->
            <tr>
              <td style="padding:16px;width:28%;vertical-align:middle">
                <img src="images/mi2v.jpg"
                     alt="MI2V"
                     style="width:100%;max-height:160px;object-fit:cover;border-radius:8px;">
              </td>
              <td style="padding:8px;width:72%;vertical-align:middle">
                <span class="papertitle">M¬≤IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering</span><br>
                Yanshu Li, Yi Cao, Hongyang He, Qisen Cheng, Xiang Fu, <strong><u>Xi Xiao</u></strong>, Tianyang Wang, Ruixiang Tang<br>
                <span style="color:#6a1b9a; font-weight:700; font-style:italic;">COLM</span>, 2025<br>
                <a href="https://openreview.net/forum?id=9ffYcEiNw9#discussion">paper</a> /
                <a href="#">code</a>
                <p></p>
                <p>
                  A novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs.
                </p>
              </td>
            </tr>





            
          </tbody></table>

          <!-- ================= PROFESSIONAL EXPERIENCE ================= -->
          <table style="width:100%;border:0;margin-top:20px;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Professional Experience</h2>

                <!-- ORNL -->
                <table style="width:100%;margin-bottom:15px;">
                  <tr>
                    <td style="width:90px;vertical-align:middle;">
                      <img src="images/ornl_logo.png"
                           style="height:80px;max-width:90px;object-fit:contain;">
                    </td>
                    <td style="vertical-align:middle;padding-left:10px;">
                      <b>Research Intern</b>, Oak Ridge National Laboratory (ORNL)<br>
                      <i>05/2025 ‚Äì Present</i><br>
                      <i>Knoxville, Tennessee, USA</i><br>
                      Working with <a href="https://www.ornl.gov/staff-profile/xiao-wang">Dr. Xiao Wang</a> on
                      large-scale climate models (e.g., ORBIT-2),  
                      efficient post-training, and exascale distributed training.
                    </td>
                  </tr>
                </table>

                <!-- UAB -->
                <table style="width:100%;margin-bottom:15px;">
                  <tr>
                    <td style="width:90px;vertical-align:middle;">
                      <img src="images/uab_logo.png"
                           style="height:80px;max-width:90px;object-fit:contain;">
                    </td>
                    <td style="vertical-align:middle;padding-left:10px;">
                      <b>Ph.D. Student</b>, University of Alabama at Birmingham (UAB)<br>
                      <i>01/2024 ‚Äì Present</i><br>
                      <i>Birmingham, Alabama, USA</i><br>
                      Advised by <a href="https://wangt0716.github.io/index.html">Prof. Tianyang Wang</a>;
                      research in efficient adaptation of large-scale models.
                    </td>
                  </tr>
                </table>

              </td>
            </tr>
          </tbody></table>

          <!-- ================= AWARDS & HONORS ================= -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Awards &amp; Honors</h2>
                <table style="width:100%;border-collapse:collapse;">
                  <tbody>
                    <tr>
                      <td style="padding:2px 0; width:110px; white-space:nowrap;"><b>2025</b></td>
                      <td style="padding:2px 0;">Best Paper Award, <b>SC 2025</b> for ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling.</td>
                    </tr>
                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>2025</b></td>
                      <td style="padding:2px 0;">Finalist, <b>ACM Gordon Bell Prize</b> (team award) for exascale climate foundation modeling on Frontier.</td>
                    </tr>
                    <tr>
                      <td style="padding:2px 0; white-space:nowrap;"><b>2025</b></td>
                      <td style="padding:2px 0;">üéâ <b>Tinker Research Grant</b> ($5,000 ) from Thinking Machines Lab to support research on efficient post-training of large-scale models.</td>
                    </tr>
                  </tbody>
                </table>
              </td>
            </tr>
          </tbody></table>

          <!-- ================= MISCELLANEA ================= -->
          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <!-- Talks & Media -->
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color:#aaba9e;">
                  <h2>Talks &amp; Media</h2>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://www.amd.com/en/blogs/2025/earth-system-modeling-with-orbit-2.html">
                  AMD story on ORBIT-2 and Frontier exascale climate modeling
                </a><br>
                More invited talks and interviews coming soon.
              </td>
            </tr>

            <!-- Academic Service -->
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color:#c6b89e;">
                  <h2>Academic Service</h2>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                Reviewer for top-venue conferences and journals including NeurIPS, ICML, CVPR, AAAI, ACM MM, TMLR, IEEE TCSVT, npj Digital Medicine.<br>
                
              </td>
            </tr>


            <div style="text-align:center; margin-top:20px;">
              <script type="text/javascript" id="clustrmaps" 
              src="//clustrmaps.com/map_v2.js?d=NBSwJ18qj5d7-eevKHYk0hX3PRgsAZBzDSD2uMKZuY0&cl=ffffff&w=250">
              </script>
            </div>



          </tbody></table>

          <!-- ================= FOOTER ================= -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's template</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </tbody></table>
  </body>
</html>
